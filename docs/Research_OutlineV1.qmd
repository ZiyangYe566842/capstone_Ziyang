---
title: "Research Outline"
author:
- Ziyang Ye
date: last-modified
bibliography: Capstone.bib
bibliographystyle: plain
format:
  html:
    css: custom.css 
    code-fold: true
    number-sections: false
    toc: true
    embed-resources: true
    theme:
      light: flatly
      dark: darkly
    fontsize: 1.1em
    linestretch: 1.7
    output-file: Research_Outline.html
    code-copy: true
    code-line-numbers: true
    highlight-style: github
---



# Introduction 

## Problem Statement 

Survival analysis, or reliability, is an area of statistics where we deal with
incomplete outcome information; that is, where we may know the time to an
outcome (death, failure of a machine part) only partially. This phenomenon is
called _censoring_, and necessitates a special set of analytic tools. These
analytic tools are used in a wide variety of domains, including healthcare and
pharmaceutical industries, engineering, marketing, and social sciences. 

Survival analytic models have typically been (semi-)parametric statistical regression
models using a special loss function. Common examples are Cox proportional
hazards regression, Weibull regression and accelerated failure time models. 
In the last years, machine learning (ML) has proven to be a great complement to 
traditional statistical methods. Indeed, several models have been developed that 
use machine learning to predict survival time in the presence of censoring. 

Most of maching learning models are
black-box models, which means they are not explainable, i.e., we cannot easily
ascertain the effect of a particular variable on the predicted outcome. However, the effect 
of each feature on the outcome is often required, especially in regulated
industries like pharma, finance and insurance, since it often needs to be
explained why an individual is predicted to have shorter survival time (higher
risk). 

In this project we will explore how to estimate the explainability or
interpretability of features in ML-based survival models, and whether this can
be related to causal inference. We will also explore the small-sample
variability of such estimates and how robust our interpretations may be. 


## Research Questions 

-  How can we evaluate the effect of individual features in ML-based survival models which are black boxes?
-  How can the effects of features be quantified, especially in the presence of
   interactions or non-linear effects?
-  How variable or robust are the effect estimates in the small-sample setting?
-  Can the estimated effects be related to causal inference concepts?


## Our work & Discussion 

We will review the existing methods for interpretable machine learning models and how they can be applied to the censored data context. We will consider methods that can account for interactions or non-linear effects, including potential extensions of SHAP and LIME. We will investigate the statistical properties of our estimates via simulated data and bootstrap methods. We will then apply our methods to a real-world survival dataset (TBD). 

# Literature Review 

More and more Machine learning models are applied to 
a variety of fields to achieve better predictive results. 
However, the majority of machine learning models lack 
interpretability.
Machine learning models are 
mostly black-box models, which raises concerns about trust and reliability.
Determining the impact of a specific variable on the 
predicted outcome is not straightforward. 
Nonetheless, understanding the influence of each 
feature on the outcome is frequently necessary, 
particularly in regulated sectors such as 
pharmaceuticals, finance, and insurance, as there is often a need 
to provide explanations for predictions, 
particularly when forecasting shorter survival times 
(indicating higher risk) for individuals.
Therefore, there is an increasing interest in interpretable machine-learning models because 
understanding the reasons behind these black-box models is important.

#### Interpretable Machine Learning

Explainable or interpretable machine learning represents a machine learning model's capacity to offer comprehensible insights into how it arrives at decisions. Models with interpretability, such as decision trees, decision rules, and linear regression, can explain predictions. The interpretability is fundamental in guaranteeing the transparency and reliability of AI and machine learning models across diverse real-world scenarios.

#### Local Surrogate (LIME)

Ribeiro(2016) proposes LIME, a local surrogate model that is used 
to explain individual predictions[@ribeiro_why_2016]. Rather than train a global surrogate model, LIME concentrates on training local surrogate models to explain individual predictions.LIME assesses the impact on predictions by introducing various data variations to the machine learning model.
LIME generates a dataset that includes perturbed samples along with 
the corresponding predictions made by the black box model.
In this new dataset, LIME proceeds to train an interpretable model, 
assigning weights based on the proximity of sampled instances. The interpretable model can be anything, such as Lasso or a decision tree. The learned model should be a good approximation of the machine learning model predictions locally.
The biggest advantage of the surrogate model is that it doesn't require 
information on the inner workings of the black box model. Only the
input dataset and outcome of the prediction are required. However, the 
big problem is the instability of the explanations. However, in an article,
Alvarez-Melis(2018) showed that the explanations of two very close 
points varied greatly in a simulated setting[@alvarez-melis_robustness_2018]. If you repeat the sampling process, 
then the explanations that come out can be different.

#### SHAP
In 2017, Lundberg proposed a Unified Approach to the Interpreting Model 
Predictions[@lundberg_unified_2017], which is named SHAP(Shapley Additive exPlanations).
In common, SHAP is based on the optimal Shapley values.
The goal of SHAP is to elucidate the prediction of an instance by assessing the impact of each feature on that prediction. This explanation method leverages Shapley values, which are derived from the principles of coalitional game theory, to compute these feature contributions.
Its novel components include: 
(1) the identification of a new class of additive feature 
importance measures
(2) theoretical results showing there is a unique solution 
in this class with a set of desirable properties.
The predictions are evenly distributed across the feature values, 
which yield contrastive explanations that compare the 
prediction with the average prediction.
if features are dependent, e.g. correlated, this leads to 
putting too much weight on valueless data points.

#### Other Explainable machine learning models
A variety of methods of explainability are important for survival analysis because survival data involve time-to-event information, censoring, and time-dependent effects. Specialized explainability methods are required to explain complex survival models, identify important features, and provide meaningful insights for healthcare, risk assessment, and clinical decision-making.

In 2020, Lundberg proposed an improved tree-based model, which has a better 
interpretability[@lundberg_local_2020].A polynomial time 
algorithm is applied to compute optimal explanations based on game theory.
Combining many local explanations of each prediction, which
also include the Shapley Value.

In a review article[@marcinkevics_interpretability_2023], Marcinkevičs(2020) mentioned an intrinsically interpretable neural network model 
that allows disentangling contributions of 
individual predictor variables or basis concepts. Self-explaining neural networks (SENN) 
[@alvarez_melis_towards_2018] are motivated by explicitness, faithfulness, and stability properties 
– three desiderata for interpretability.SENNs show a straightforward local model 
behavior with high complexity and nonlinearity on a global scale. The regressors 
serve as effect modifiers in their own right, and the framework is enhanced 
with interpretable foundational concepts that are defined based on the raw inputs.

#### Interpretable machine learning in a survival context

Several machine learning models have been proposed in the
survival context. Many research papers piqued our interest in this topic.
In 2021, Moncada‑Torres wrote an article that discusses the application 
of how an explainable machine learning model can outperform the Cox regression 
predictions in breast cancer survival[@moncada-torres_explainable_2021].
Giunchiglia(2018) proposed a new recurrent neural network model 
for personalized survival analysis called rnn-surv[@kurkova_rnn-surv_2018].
At each time step, the network receives input that includes the patient's 
characteristics and the time step identifier. It generates 
an embedding and outputs the survival function's value. 
Finally, these survival function values are combined linearly to calculate the singular risk score.
Césaire J. K. Fouodo(2018) describe survival support vector machines[@fouodo_support_2018] and their implementation, 
provide examples and compare the prediction performance with the Cox proportional hazards model, 
random survival forests and gradient boosting using several real datasets.Sonabend[@sonabend2021mlr3proba] provide 
an R package for machine learning in survival analysis called mlr3proba.
Baoshan Ma(2022) proposed an improved survival prediction model based on the XGBoost framework called XGBLC[@ma_xgblc_2022], 
which used Lasso-Cox to enhance the ability to analyze high-dimensional genomic data.
Shi Hu(2021) proposed a new Transformer-based survival model that estimates the patient-specific survival distribution.
Using ordinal regression to optimize the survival probabilities over time, and penalize randomized discordant pairs[@hu_transformer-based_2021].
A lot of numerical experiments demonstrate the efficiency of the application of interpretable machine learning in a survival context.

#### Methods for interpretable machine learning models
There are several proposed methods for interpretable machine learning models in
survival analysis. Maxim S. Kovalev(2020) proposed a new method called SurvLIME[@kovalev_survlime_2020] 
for explaining the machine learning survival model, which can be considered an extension 
or modification of the well-known method LIME.
Mateusz Krzyziński(2023) introduces SurvSHAP(t), the first time-dependent explanation 
that allows for interpreting survival black-box models[@krzyzinski_survshapt_2023]. 
It is based on Shapley Additive Explanations with solid 
theoretical foundations and a broad adoption among machine learning practitioners.
### Some application
Interpretable machine learning is adopted as a useful tool in a variety of domains.
Lei Cui（2020） has proposed a survival analysis system that takes advantage of recently emerging deep learning
techniques. Extensive experiments show that the proposed survival model has excellent predictive power for a public
lung cancer dataset in terms of two commonly used metrics: the log-rank test (p-value) of the
Kaplan-Meier estimate and concordance index [@cui_deep_2020].

Currently, many researchers have proposed several useful methods and improved models to explain the 
black box model. However, they don't provide a useful way to measure and quantify the efficiency of
these interpretable models. We need to find a solid way to select the best interpretable model. In our work,
we will do a comparative study of these models.


# Method 

##  

I will finish this paper under the guidance of Dr .Abhijit Das Gupta.

##  

Methods for variable importance and interpretation in machine learning models
range from permutation-based variable importance scores to more model-agnostic
methods like LIME and SHAP. We wil review these methods and propose extensions
especially in the context of interactions. This will improve our understanding
of how flexible machine learning methods that often provide improved outcome
prediction can also be utilized for feature interpretation. Quantitative methods
will be supplemented with graphical methods like partial dependence plots and
other visualizations to help understand how single or multiple features can
affect survival outcome and the relative importance of the features. 

## Data collection 

We will use simulated survival data to evaluate the statistical properties of
different interpretable machine learning methods for survival analysis.
Simulations can be run using, among other tools, the R package `survsim`. We
will also identify several real-world survival datasets with varying numbers of
features to see how interpretable ML models perform in practice, with
uncertainty being evaluated via bootstrap resampling. 

### some dataset

Sleepy Driver EEG Brainwave Data
We collected EEG signal data from 4 drivers while they were awake and asleep. For safety precautions they weren't actually driving while acquiring the signals.
(https://www.kaggle.com/datasets/naddamuhhamed/sleepy-driver-eeg-brainwave-data/code)


Credit Card Fraud（https://www.openml.org/d/1597）
The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset present transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.






# Results  (Expected)

We expect to provide recommendations about methods to use for interpreting
ML-based survival models, as well as graphical tools to visualize the marginal
and joint effects of different features on survival.  
