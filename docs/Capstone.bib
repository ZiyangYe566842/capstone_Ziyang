
@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2023-09-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\13194\\Zotero\\storage\\HY2IMQ92\\Lundberg 和 Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@article{moncada-torres_explainable_2021,
	title = {Explainable machine learning can outperform {Cox} regression predictions and provide insights in breast cancer survival},
	volume = {11},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-86327-7},
	doi = {10.1038/s41598-021-86327-7},
	abstract = {Abstract
            
              Cox Proportional Hazards (CPH) analysis is the standard for survival analysis in oncology. Recently, several machine learning (ML) techniques have been adapted for this task. Although they have shown to yield results at least as good as classical methods, they are often disregarded because of their lack of transparency and little to no explainability, which are key for their adoption in clinical settings. In this paper, we used data from the Netherlands Cancer Registry of 36,658 non-metastatic breast cancer patients to compare the performance of CPH with ML techniques (Random Survival Forests, Survival Support Vector Machines, and Extreme Gradient Boosting [XGB]) in predicting survival using the
              
                
                  \$\$c\$\$
                  
                    c
                  
                
              
              -index. We demonstrated that in our dataset, ML-based models can perform at least as good as the classical CPH regression (
              
                
                  \$\$c\$\$
                  
                    c
                  
                
              
              -index 
              
                
                  \$\${\textbackslash}sim {\textbackslash},0.63\$\$
                  
                    
                      ∼
                      
                      0.63
                    
                  
                
              
              ), and in the case of XGB even better (
              
                
                  \$\$c\$\$
                  
                    c
                  
                
              
              -index 
              
                
                  \$\${\textbackslash}sim 0.73\$\$
                  
                    
                      ∼
                      0.73
                    
                  
                
              
              ). Furthermore, we used Shapley Additive Explanation (SHAP) values to explain the models’ predictions. We concluded that the difference in performance can be attributed to XGB’s ability to model nonlinearities and complex interactions. We also investigated the impact of specific features on the models’ predictions as well as their corresponding insights. Lastly, we showed that explainable ML can generate explicit knowledge of how models make their predictions, which is crucial in increasing the trust and adoption of innovative ML techniques in oncology and healthcare overall.},
	language = {en},
	number = {1},
	urldate = {2023-09-25},
	journal = {Scientific Reports},
	author = {Moncada-Torres, Arturo and Van Maaren, Marissa C. and Hendriks, Mathijs P. and Siesling, Sabine and Geleijnse, Gijs},
	month = mar,
	year = {2021},
	pages = {6968},
	file = {Moncada-Torres 等 - 2021 - Explainable machine learning can outperform Cox re.pdf:C\:\\Users\\13194\\Zotero\\storage\\DXC4K38W\\Moncada-Torres 等 - 2021 - Explainable machine learning can outperform Cox re.pdf:application/pdf},
}

@misc{alvarez-melis_robustness_2018,
	title = {On the {Robustness} of {Interpretability} {Methods}},
	url = {http://arxiv.org/abs/1806.08049},
	doi = {10.48550/arXiv.1806.08049},
	abstract = {We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	month = jun,
	year = {2018},
	note = {arXiv:1806.08049 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: presented at 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden},
	file = {arXiv Fulltext PDF:C\:\\Users\\13194\\Zotero\\storage\\54W3P9UB\\Alvarez-Melis 和 Jaakkola - 2018 - On the Robustness of Interpretability Methods.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13194\\Zotero\\storage\\RFU2GENF\\1806.html:text/html},
}

@article{lundberg_local_2020,
	title = {From local explanations to global understanding with explainable {AI} for trees},
	volume = {2},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0138-9},
	doi = {10.1038/s42256-019-0138-9},
	language = {en},
	number = {1},
	urldate = {2023-09-25},
	journal = {Nature Machine Intelligence},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	month = jan,
	year = {2020},
	pages = {56--67},
	file = {Lundberg 等 - 2020 - From local explanations to global understanding wi.pdf:C\:\\Users\\13194\\Zotero\\storage\\AR89W5GL\\Lundberg 等 - 2020 - From local explanations to global understanding wi.pdf:application/pdf},
}

@misc{marcinkevics_interpretability_2023,
	title = {Interpretability and {Explainability}: {A} {Machine} {Learning} {Zoo} {Mini}-tour},
	shorttitle = {Interpretability and {Explainability}},
	url = {http://arxiv.org/abs/2012.01805},
	doi = {10.48550/arXiv.2012.01805},
	abstract = {In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and statistical applications in medicine, economics, law, and natural sciences. Although interpretability and explainability have escaped a clear universal definition, many techniques motivated by these properties have been developed over the recent 30 years with the focus currently shifting towards deep learning methods. In this review, we emphasise the divide between interpretability and explainability and illustrate these two different research directions with concrete examples of the state-of-the-art. The review is intended for a general machine learning audience with interest in exploring the problems of interpretation and explanation beyond logistic regression or random forest variable importance. This work is not an exhaustive literature survey, but rather a primer focusing selectively on certain lines of research which the authors found interesting or informative.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Marcinkevičs, Ričards and Vogt, Julia E.},
	month = mar,
	year = {2023},
	note = {arXiv:2012.01805 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: A preprint version of the 2023 WIREs Data Mining and Knowledge Discovery article},
	file = {arXiv Fulltext PDF:C\:\\Users\\13194\\Zotero\\storage\\PZ37VAYC\\Marcinkevičs 和 Vogt - 2023 - Interpretability and Explainability A Machine Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13194\\Zotero\\storage\\R8879URY\\2012.html:text/html},
}

@inproceedings{alvarez_melis_towards_2018,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	urldate = {2023-09-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alvarez Melis, David and Jaakkola, Tommi},
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\13194\\Zotero\\storage\\24MKU4FJ\\Alvarez Melis 和 Jaakkola - 2018 - Towards Robust Interpretability with Self-Explaini.pdf:application/pdf},
}

@incollection{kurkova_rnn-surv_2018,
	address = {Cham},
	title = {{RNN}-{SURV}: {A} {Deep} {Recurrent} {Model} for {Survival} {Analysis}},
	volume = {11141},
	isbn = {978-3-030-01423-0 978-3-030-01424-7},
	shorttitle = {{RNN}-{SURV}},
	url = {http://link.springer.com/10.1007/978-3-030-01424-7_3},
	abstract = {Current medical practice is driven by clinical guidelines which are designed for the “average” patient. Deep learning is enabling medicine to become personalized to the patient at hand. In this paper we present a new recurrent neural network model for personalized survival analysis called rnn-surv. Our model is able to exploit censored data to compute both the risk score and the survival function of each patient. At each time step, the network takes as input the features characterizing the patient and the identiﬁer of the time step, creates an embedding, and outputs the value of the survival function in that time step. Finally, the values of the survival function are linearly combined to compute the unique risk score. Thanks to the model structure and the training designed to exploit two loss functions, our model gets better concordance index (C-index) than the state of the art approaches.},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Giunchiglia, Eleonora and Nemchenko, Anton and Van Der Schaar, Mihaela},
	editor = {Kůrková, Věra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2018},
	doi = {10.1007/978-3-030-01424-7_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {23--32},
	file = {Giunchiglia 等 - 2018 - RNN-SURV A Deep Recurrent Model for Survival Anal.pdf:C\:\\Users\\13194\\Zotero\\storage\\CTF6G5SZ\\Giunchiglia 等 - 2018 - RNN-SURV A Deep Recurrent Model for Survival Anal.pdf:application/pdf},
}

@article{fouodo_support_2018,
	title = {Support {Vector} {Machines} for {Survival} {Analysis} with {R}},
	volume = {10},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2018/RJ-2018-005/index.html},
	doi = {10.32614/RJ-2018-005},
	abstract = {This article introduces the R package survivalsvm, implementing support vector machines for survival analysis. Three approaches are available in the package: The regression approach takes censoring into account when formulating the inequality constraints of the support vector problem. In the ranking approach, the inequality constraints set the objective to maximize the concordance index for comparable pairs of observations. The hybrid approach combines the regression and ranking constraints in a single model. We describe survival support vector machines and their implementation, provide examples and compare the prediction performance with the Cox proportional hazards model, random survival forests and gradient boosting using several real datasets. On these datasets, survival support vector machines perform on par with the reference methods.},
	language = {en},
	number = {1},
	urldate = {2023-09-25},
	journal = {The R Journal},
	author = {Fouodo, J., K., Césaire and König, R., Inke and Weihs, Claus and Ziegler, Andreas and Wright, N., Marvin},
	year = {2018},
	pages = {412},
	file = {Fouodo 等 - 2018 - Support Vector Machines for Survival Analysis with.pdf:C\:\\Users\\13194\\Zotero\\storage\\3J4P6HW8\\Fouodo 等 - 2018 - Support Vector Machines for Survival Analysis with.pdf:application/pdf},
}

@article{ma_xgblc_2022,
	title = {{XGBLC}: an improved survival prediction model based on {XGBoost}},
	volume = {38},
	issn = {1367-4803, 1367-4811},
	shorttitle = {{XGBLC}},
	url = {https://academic.oup.com/bioinformatics/article/38/2/410/6377771},
	doi = {10.1093/bioinformatics/btab675},
	abstract = {Motivation: Survival analysis using gene expression proﬁles plays a crucial role in the interpretation of clinical research and assessment of disease therapy programs. Several prediction models have been developed to explore the relationship between patients’ covariates and survival. However, the high-dimensional genomic features limit the prediction performance of the survival model. Thus, an accurate and reliable prediction model is necessary for survival analysis using high-dimensional genomic data.},
	language = {en},
	number = {2},
	urldate = {2023-09-25},
	journal = {Bioinformatics},
	author = {Ma, Baoshan and Yan, Ge and Chai, Bingjie and Hou, Xiaoyu},
	editor = {Boeva, Valentina},
	month = jan,
	year = {2022},
	pages = {410--418},
	file = {Ma 等 - 2022 - XGBLC an improved survival prediction model based.pdf:C\:\\Users\\13194\\Zotero\\storage\\XK98H4DY\\Ma 等 - 2022 - XGBLC an improved survival prediction model based.pdf:application/pdf},
}

@inproceedings{hu_transformer-based_2021,
	title = {Transformer-{Based} {Deep} {Survival} {Analysis}},
	url = {https://proceedings.mlr.press/v146/hu21a.html},
	abstract = {In this work, we propose a new Transformer-based survival model which estimates the patient-specific survival distribution. Our contributions are twofold. First, to the best of our knowledge, existing deep survival models use either fully connected or recurrent networks, and we are the first to apply the Transformer in survival analysis. In addition, we use ordinal regression to optimize the survival probabilities over time, and penalize randomized discordant pairs. Second, many survival models are evaluated using only the ranking metrics such as the concordance index. We propose to also use the absolute error metric that evaluates the precise duration predictions on observed subjects. We demonstrate our model on two publicly available real-world datasets, and show that our mean absolute error results are significantly better than the current models, meanwhile, it is challenging to determine the best model under the concordance index.},
	language = {en},
	urldate = {2023-09-25},
	booktitle = {Proceedings of {AAAI} {Spring} {Symposium} on {Survival} {Prediction} - {Algorithms}, {Challenges}, and {Applications} 2021},
	publisher = {PMLR},
	author = {Hu, Shi and Fridgeirsson, Egill and Wingen, Guido van and Welling, Max},
	month = may,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {132--148},
	file = {Full Text PDF:C\:\\Users\\13194\\Zotero\\storage\\ERWIFPA9\\Hu 等 - 2021 - Transformer-Based Deep Survival Analysis.pdf:application/pdf},
}

@misc{kovalev_survlime_2020,
	title = {{SurvLIME}: {A} method for explaining machine learning survival models},
	shorttitle = {{SurvLIME}},
	url = {http://arxiv.org/abs/2003.08371},
	doi = {10.48550/arXiv.2003.08371},
	abstract = {A new method called SurvLIME for explaining machine learning survival models is proposed. It can be viewed as an extension or modification of the well-known method LIME. The main idea behind the proposed method is to apply the Cox proportional hazards model to approximate the survival model at the local area around a test example. The Cox model is used because it considers a linear combination of the example covariates such that coefficients of the covariates can be regarded as quantitative impacts on the prediction. Another idea is to approximate cumulative hazard functions of the explained model and the Cox model by using a set of perturbed points in a local area around the point of interest. The method is reduced to solving an unconstrained convex optimization problem. A lot of numerical experiments demonstrate the SurvLIME efficiency.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Kovalev, Maxim S. and Utkin, Lev V. and Kasimov, Ernest M.},
	month = mar,
	year = {2020},
	note = {arXiv:2003.08371 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13194\\Zotero\\storage\\SJ9UZV63\\Kovalev 等 - 2020 - SurvLIME A method for explaining machine learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13194\\Zotero\\storage\\6KBJ4YFN\\2003.html:text/html},
}

@article{krzyzinski_survshapt_2023,
	title = {{SurvSHAP}(t): {Time}-dependent explanations of machine learning survival models},
	volume = {262},
	issn = {09507051},
	shorttitle = {{SurvSHAP}(t)},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705122013302},
	doi = {10.1016/j.knosys.2022.110234},
	abstract = {Machine and deep learning survival models demonstrate similar or even improved time-to-event prediction capabilities compared to classical statistical learning methods yet are too complex to be interpreted by humans. Several model-agnostic explanations are available to overcome this issue; however, none directly explain the survival function prediction. In this paper, we introduce SurvSHAP(t), the first time-dependent explanation that allows for interpreting survival black-box models. It is based on SHapley Additive exPlanations with solid theoretical foundations and a broad adoption among machine learning practitioners. The proposed methods aim to enhance precision diagnostics and support domain experts in making decisions. Experiments on synthetic and medical data confirm that SurvSHAP(t) can detect variables with a time-dependent effect, and its aggregation is a better determinant of the importance of variables for a prediction than SurvLIME. SurvSHAP(t) is model-agnostic and can be applied to all models with functional output. We provide an accessible implementation of time-dependent explanations in Python at https://github.com/MI2DataLab/survshap.},
	language = {en},
	urldate = {2023-09-25},
	journal = {Knowledge-Based Systems},
	author = {Krzyziński, Mateusz and Spytek, Mikołaj and Baniecki, Hubert and Biecek, Przemysław},
	month = feb,
	year = {2023},
	pages = {110234},
	file = {Krzyziński 等 - 2023 - SurvSHAP(t) Time-dependent explanations of machin.pdf:C\:\\Users\\13194\\Zotero\\storage\\Z2TASST4\\Krzyziński 等 - 2023 - SurvSHAP(t) Time-dependent explanations of machin.pdf:application/pdf},
}

@article{cui_deep_2020,
	title = {A deep learning-based framework for lung cancer survival analysis with biomarker interpretation},
	volume = {21},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3431-z},
	doi = {10.1186/s12859-020-3431-z},
	abstract = {Background: Lung cancer is the leading cause of cancer-related deaths in both men and women in the United States, and it has a much lower five-year survival rate than many other cancers. Accurate survival analysis is urgently needed for better disease diagnosis and treatment management.
Results: In this work, we propose a survival analysis system that takes advantage of recently emerging deep learning techniques. The proposed system consists of three major components. 1) The first component is an end-to-end cellular feature learning module using a deep neural network with global average pooling. The learned cellular representations encode high-level biologically relevant information without requiring individual cell segmentation, which is aggregated into patient-level feature vectors by using a locality-constrained linear coding (LLC)-based bag of words (BoW) encoding algorithm. 2) The second component is a Cox proportional hazards model with an elastic net penalty for robust feature selection and survival analysis. 3) The third commponent is a biomarker interpretation module that can help localize the image regions that contribute to the survival model’s decision. Extensive experiments show that the proposed survival model has excellent predictive power for a public (i.e., The Cancer Genome Atlas) lung cancer dataset in terms of two commonly used metrics: log-rank test (p-value) of the Kaplan-Meier estimate and concordance index (c-index).
Conclusions: In this work, we have proposed a segmentation-free survival analysis system that takes advantage of the recently emerging deep learning framework and well-studied survival analysis methods such as the Cox proportional hazards model. In addition, we provide an approach to visualize the discovered biomarkers, which can serve as concrete evidence supporting the survival model’s decision.},
	language = {en},
	number = {1},
	urldate = {2023-09-28},
	journal = {BMC Bioinformatics},
	author = {Cui, Lei and Li, Hansheng and Hui, Wenli and Chen, Sitong and Yang, Lin and Kang, Yuxin and Bo, Qirong and Feng, Jun},
	month = dec,
	year = {2020},
	pages = {112},
	file = {Cui 等 - 2020 - A deep learning-based framework for lung cancer su.pdf:C\:\\Users\\13194\\Zotero\\storage\\AK2FB4V3\\Cui 等 - 2020 - A deep learning-based framework for lung cancer su.pdf:application/pdf},
}

@article{hao_interpretable_2019,
	title = {Interpretable deep neural network for cancer survival analysis by integrating genomic and clinical data},
	volume = {12},
	issn = {1755-8794},
	url = {https://bmcmedgenomics.biomedcentral.com/articles/10.1186/s12920-019-0624-2},
	doi = {10.1186/s12920-019-0624-2},
	abstract = {Background: Understanding the complex biological mechanisms of cancer patient survival using genomic and clinical data is vital, not only to develop new treatments for patients, but also to improve survival prediction. However, highly nonlinear and high-dimension, low-sample size (HDLSS) data cause computational challenges to applying conventional survival analysis.
Results: We propose a novel biologically interpretable pathway-based sparse deep neural network, named Cox-PASNet, which integrates high-dimensional gene expression data and clinical data on a simple neural network architecture for survival analysis. Cox-PASNet is biologically interpretable where nodes in the neural network correspond to biological genes and pathways, while capturing the nonlinear and hierarchical effects of biological pathways associated with cancer patient survival. We also propose a heuristic optimization solution to train Cox-PASNet with HDLSS data. Cox-PASNet was intensively evaluated by comparing the predictive performance of current state-of-the-art methods on glioblastoma multiforme (GBM) and ovarian serous cystadenocarcinoma (OV) cancer. In the experiments, Cox-PASNet showed out-performance, compared to the benchmarking methods. Moreover, the neural network architecture of Cox-PASNet was biologically interpreted, and several significant prognostic factors of genes and biological pathways were identified.
Conclusions: Cox-PASNet models biological mechanisms in the neural network by incorporating biological pathway databases and sparse coding. The neural network of Cox-PASNet can identify nonlinear and hierarchical associations of genomic and clinical data to cancer patient survival. The open-source code of Cox-PASNet in PyTorch implemented for training, evaluation, and model interpretation is available at: https://github.com/DataX-JieHao/Cox-PASNet.},
	language = {en},
	number = {S10},
	urldate = {2023-09-28},
	journal = {BMC Medical Genomics},
	author = {Hao, Jie and Kim, Youngsoon and Mallavarapu, Tejaswini and Oh, Jung Hun and Kang, Mingon},
	month = dec,
	year = {2019},
	pages = {189},
	file = {Hao 等 - 2019 - Interpretable deep neural network for cancer survi.pdf:C\:\\Users\\13194\\Zotero\\storage\\JW7QAJLE\\Hao 等 - 2019 - Interpretable deep neural network for cancer survi.pdf:application/pdf},
}


@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13194\\Zotero\\storage\\6NTR2XFA\\Ribeiro 等 - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13194\\Zotero\\storage\\67WWM2TA\\1602.html:text/html},
}
@article{sonabend2021mlr3proba,
  title={mlr3proba: an R package for machine learning in survival analysis},
  author={Sonabend, Raphael and Kir{\'a}ly, Franz J and Bender, Andreas and Bischl, Bernd and Lang, Michel},
  journal={Bioinformatics},
  volume={37},
  number={17},
  pages={2789--2791},
  year={2021},
  publisher={Oxford University Press}
}